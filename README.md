# Анализ текстовых данных

## Тема
"Откуда берутся датасеты? Практический проект по сбору данных и работе с текстами"

## Цель проекта
В этом проекте реализован процесс создания собственного датасета путем парсинга веб-страниц и последующего применения методов обработки естественного языка (NLP) для анализа собранных данных.

## Что было сделано

### Часть 1: Парсинг
- Разработан парсер для сбора данных с веб-сайта
- Реализована обработка HTML-страниц с соблюдением задержек между запросами
- Собраны текстовые описания и целевые переменные
- Данные сохранены в структурированном формате

### Часть 2: NLP Анализ

#### Данные
Поскольку собранных через парсинг данных оказалось недостаточно для качественного анализа, было принято решение дополнительно использовать датасет IMDB Movie Reviews с Kaggle (https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews). Этот датасет содержит 50,000 отзывов на фильмы с бинарной семантической оценкой ("позитивный" или "негативный").

#### Этапы обработки:
1. Предобработка текста:
   - Токенизация
   - Удаление стоп-слов
   - Лемматизация/стемминг

2. Создание признаков:
   - TF-IDF преобразование
   - Использование биграмм
   - Фильтрация редких и частых слов (настройка параметров min/max_df)
   - Отключение l2 регуляризации

3. Построение модели:
   - Разделение данных на train/test (70/30)
   - Применение регрессии для непрерывных целевых переменных
   - Оценка качества модели
   - Визуализация коэффициентов регрессии (топ-50 слов)

## Использованные технологии
- Beautiful Soup 4 (парсинг HTML)
- NLTK (обработка текста)
- Scikit-learn (машинное обучение)
- Pandas (обработка данных)
- Matplotlib/Seaborn (визуализация)

## Файлы
- `parsing.ipynb` - Jupyter notebook с реализацией парсера
- `nlp_analysis.ipynb` - Jupyter notebook с NLP анализом данных

